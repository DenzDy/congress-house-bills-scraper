{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e947ce97",
   "metadata": {},
   "source": [
    "# Congress House Bills Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39836aad",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab104ae",
   "metadata": {},
   "source": [
    "A web scraper for Congress House bills (HBNXXXXX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7153d4",
   "metadata": {},
   "source": [
    "## Imports and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b60d2",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6990c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m69 packages\u001b[0m \u001b[2min 116ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c375f3",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b39ab126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright, Page, Browser\n",
    "from playwright_stealth import Stealth\n",
    "from camoufox.async_api import AsyncCamoufox\n",
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "MAX_CONCURRENCY = 5\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7f814",
   "metadata": {},
   "source": [
    "## Helper and Utility Functions and Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f4617",
   "metadata": {},
   "source": [
    "### File Class Object Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbbca32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class File:\n",
    "    def __init__(\n",
    "            self,\n",
    "            hbn : str,\n",
    "            main_title : str, \n",
    "            session_number : str, \n",
    "            significance : str, \n",
    "            date_filed : str, \n",
    "            principal_authors : str, \n",
    "            date_read : str, \n",
    "            primary_referral : str, \n",
    "            bill_status : str,  \n",
    "            text_filed : str, \n",
    "            is_file_downloadable : str\n",
    "            ):\n",
    "        self.hbn = hbn\n",
    "        self.main_title = main_title\n",
    "        self.session_number = session_number\n",
    "        self.significance = significance\n",
    "        self.date_filed = date_filed\n",
    "        self.principal_authors = principal_authors\n",
    "        self.date_read = date_read\n",
    "        self.primary_referral = primary_referral\n",
    "        self.bill_status = bill_status\n",
    "        self.text_filed = text_filed\n",
    "        self.is_file_downloadable = is_file_downloadable\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, File):\n",
    "            print(\"Collision Check\")\n",
    "            return self.hbn == other.hbn\n",
    "        return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.hbn)\n",
    "files : set[File] = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7abba",
   "metadata": {},
   "source": [
    "### JSON Encoder for File Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b263247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_encoder(obj: File):\n",
    "    if isinstance(obj, File):\n",
    "        return {\n",
    "            'House Bill Number' : obj.hbn,\n",
    "            'Main Title' : obj.main_title,\n",
    "            'Session Number' : obj.session_number,\n",
    "            'Significance' : obj.significance,\n",
    "            'Date Filed' : obj.date_filed,\n",
    "            'Principal Authors' : obj.principal_authors,\n",
    "            'Date Read' : obj.date_read,\n",
    "            'Primary Referral' : obj.primary_referral,\n",
    "            'Bill Status' : obj.bill_status,\n",
    "            'Text Filed' : obj.text_filed\n",
    "        }\n",
    "    raise TypeError(\"Object is not JSON parsable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fc34a",
   "metadata": {},
   "source": [
    "### Download File From URL Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08df14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, dest_folder: str):\n",
    "    \"\"\"\n",
    "    Downloads the file from the URL provided and places it in the destination folder provided\n",
    "\n",
    "    Inputs:\n",
    "    url (str): input URL of file\n",
    "    dest_folder (str): destination folder/directory of downloaded file\n",
    "    \n",
    "    Outputs:\n",
    "    Returns 1 if the download was successful, and 0 if not.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)  # create folder if it does not exist\n",
    "    try:\n",
    "        filename = url.split('/')[-1].replace(\" \", \"_\")  # be careful with file names\n",
    "        file_path = os.path.join(dest_folder, filename)\n",
    "        # print(f\"URL: {url}\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.ok:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 8):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        f.flush()\n",
    "                        os.fsync(f.fileno())\n",
    "        else:  # HTTP status code 4XX/5XX\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370dff8",
   "metadata": {},
   "source": [
    "### Get Files from Current Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8dc60a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_files_from_page(hb_items_locator):\n",
    "    count = await hb_items_locator.count()\n",
    "    for i in range(count):\n",
    "        hb_item = hb_items_locator.nth(i)\n",
    "        \n",
    "        # Trigger AOS animation\n",
    "        await hb_item.scroll_into_view_if_needed()\n",
    "\n",
    "        # 1. Capture Header Info\n",
    "        # Using .inner_text() on the specific span to get a clean string\n",
    "        hbn = await hb_item.locator(\"span.rounded.border span span\").first.inner_text()\n",
    "        main_title = await hb_item.locator(\"span.text-blue-500\").first.inner_text()\n",
    "        print(f\"Index {i}: {hbn.strip()}\")\n",
    "\n",
    "        # 2. Robust Metadata Helper\n",
    "        # This specifically targets the grid container so it cannot 'leak' to the footer\n",
    "        async def get_meta(label):\n",
    "            try:\n",
    "                # Logic: Find the label div, then get the very next div sibling (+)\n",
    "                # only if it is inside the grid container\n",
    "                value_locator = hb_item.locator(\".grid.gap-1.px-5\") \\\n",
    "                                       .locator(f\"div:has-text('{label}') + div\")\n",
    "                \n",
    "                text = await value_locator.first.inner_text(timeout=1000)\n",
    "                return text.strip()\n",
    "            except:\n",
    "                return \"N/A\"\n",
    "\n",
    "        # 3. PDF Link Fix: Use .first to avoid strictness errors\n",
    "        pdf_loc = hb_item.locator('a[href$=\".pdf\"]').first\n",
    "        link = await pdf_loc.get_attribute('href') if await pdf_loc.count() > 0 else \"N/A\"\n",
    "        downloadability = False\n",
    "        if link != 'N/A':\n",
    "            downloadability = download(link, \"outputs/\")\n",
    "        # 4. Build File Object\n",
    "        # Note: Ensure you are passing the strings, not the locator objects\n",
    "        new_file = File(\n",
    "            hbn.strip(),\n",
    "            main_title.strip(),\n",
    "            await get_meta(\"Session No. :\"),\n",
    "            await get_meta(\"Significance :\"),\n",
    "            await get_meta(\"Date Filed :\"),\n",
    "            await get_meta(\"Principal Author/s :\"),\n",
    "            await get_meta(\"Date Read :\"),\n",
    "            await get_meta(\"Primary Referral :\"),\n",
    "            await get_meta(\"Bill Status :\"),\n",
    "            link,\n",
    "            downloadability # Downloadable\n",
    "        )\n",
    "        files.add(new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75642a",
   "metadata": {},
   "source": [
    "## Scraper Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a4dc58",
   "metadata": {},
   "source": [
    "### Proxy Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be76ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY_SERVER = \"http://84.17.47.149:9002\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ec883",
   "metadata": {},
   "source": [
    "### Actual Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b5b3c",
   "metadata": {},
   "source": [
    "#### File Metadata Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76f815bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f514dbc",
   "metadata": {},
   "source": [
    "#### Scraper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89fca00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred. Saving progress...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# --- Processing logic (e.g., saving to JSON) ---\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33moutputs/metadata.json\u001b[39m\u001b[33m'\u001b[39m, mode=\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[43mjson\u001b[49m.dump(\n\u001b[32m     87\u001b[39m         obj=\u001b[38;5;28mlist\u001b[39m(files),\n\u001b[32m     88\u001b[39m         fp=f,\n\u001b[32m     89\u001b[39m         default=json_encoder,\n\u001b[32m     90\u001b[39m         indent=\u001b[32m4\u001b[39m\n\u001b[32m     91\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    async with AsyncCamoufox(headless=False, geoip=True) as browser:\n",
    "        context = await browser.new_context(viewport={\"width\":1000, \"height\":500})\n",
    "        page = await context.new_page()\n",
    "\n",
    "        await page.goto(\"https://congress.gov.ph/legislative-documents/\")\n",
    "        \n",
    "        # Wait for initial load\n",
    "        await page.wait_for_selector('[id=\"20th Congress\"]', state='visible', timeout=90000)\n",
    "        \n",
    "        # Set pagination to 100\n",
    "        await page.locator(\"select.form-select\").nth(1).select_option('100')    \n",
    "        \n",
    "        # Open the section\n",
    "        await page.locator('[id=\"20th Congress\"]').click()\n",
    "        \n",
    "        # Initial scroll and wait for first page items\n",
    "        await page.wait_for_selector('.cursor-pointer.rounded-sm.border', state='visible')\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\n",
    "        # Skip pages if needed based on files array\n",
    "        idx = 1\n",
    "        for i in range(0, int(len(files) / 100)):\n",
    "            print(f\"Skipping page {i}\\n\")\n",
    "            old_bill_id = await page.locator(\".cursor-pointer span.rounded.border span span\").first.inner_text()\n",
    "            next_button = page.locator('li.next:not(.disabled) a') # Specifically target the 'Next' link\n",
    "            await next_button.click()\n",
    "            try:\n",
    "                await page.wait_for_function(\n",
    "                    f\"\"\"() => {{\n",
    "                        const el = document.querySelector(\".cursor-pointer span.rounded.border span span\");\n",
    "                        return el && el.innerText.trim() !== \"{old_bill_id.strip()}\";\n",
    "                    }}\"\"\",\n",
    "                    timeout=15000 # 15 seconds is usually enough for a data swap\n",
    "                )\n",
    "            except:\n",
    "                # Fallback if JS check fails: wait for network to settle\n",
    "                await page.wait_for_load_state(\"networkidle\")\n",
    "            \n",
    "            # Small buffer for the UI to stabilize\n",
    "            await page.wait_for_timeout(3000)\n",
    "            idx += 1\n",
    "\n",
    "        while(idx <= 79):\n",
    "            # 1. Scrape the current page\n",
    "            hb_items_locator = page.locator('.cursor-pointer.rounded-sm.border')\n",
    "            await get_files_from_page(hb_items_locator)\n",
    "            \n",
    "            print(f\"Finished scraping page {idx}\")\n",
    "\n",
    "            # 2. Prepare for Page Turn\n",
    "            next_button = page.locator('li.next:not(.disabled) a') # Specifically target the 'Next' link\n",
    "            \n",
    "            if await next_button.count() > 0:\n",
    "                # Capture ID of the first item to track when the data actually changes\n",
    "                old_bill_id = await page.locator(\".cursor-pointer span.rounded.border span span\").first.inner_text()\n",
    "                \n",
    "                # 3. Perform Click\n",
    "                await next_button.click()\n",
    "\n",
    "                # 4. Wait for Content Swap (Simplified to avoid TimeoutError)\n",
    "                # We only wait for the text to be DIFFERENT from the old one.\n",
    "                try:\n",
    "                    await page.wait_for_function(\n",
    "                        f\"\"\"() => {{\n",
    "                            const el = document.querySelector(\".cursor-pointer span.rounded.border span span\");\n",
    "                            return el && el.innerText.trim() !== \"{old_bill_id.strip()}\";\n",
    "                        }}\"\"\",\n",
    "                        timeout=15000 # 15 seconds is usually enough for a data swap\n",
    "                    )\n",
    "                except:\n",
    "                    # Fallback if JS check fails: wait for network to settle\n",
    "                    await page.wait_for_load_state(\"networkidle\")\n",
    "                \n",
    "                # Small buffer for the UI to stabilize\n",
    "                await page.wait_for_timeout(3000)\n",
    "                idx += 1\n",
    "            else:\n",
    "                print(\"No more pages available.\")\n",
    "                break\n",
    "except:\n",
    "    print(\"Error occurred. Saving progress...\")\n",
    "\n",
    "# --- Processing logic (e.g., saving to JSON) ---\n",
    "with open('outputs/metadata.json', mode='w', encoding='utf-8') as f:\n",
    "    json.dump(\n",
    "        obj=list(files),\n",
    "        fp=f,\n",
    "        default=json_encoder,\n",
    "        indent=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20301f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congress-house-bills-scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
