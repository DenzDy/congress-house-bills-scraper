{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e947ce97",
   "metadata": {},
   "source": [
    "# Congress House Bills Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39836aad",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab104ae",
   "metadata": {},
   "source": [
    "A web scraper for Congress House bills (HBNXXXXX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7153d4",
   "metadata": {},
   "source": [
    "## Imports and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b60d2",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6990c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m71 packages\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c375f3",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39ab126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright, Page, Browser\n",
    "from camoufox.async_api import AsyncCamoufox\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1ee89",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "418729f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_BUCKET_DATA_LOCATION = os.getenv(\"AWS_BUCKET_DATA_LOCATION\")\n",
    "AWS_BUCKET_METADATA_LOCATION = os.getenv(\"AWS_BUCKET_METADATA_LOCATION\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7f814",
   "metadata": {},
   "source": [
    "## Helper and Functions and Class Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f4617",
   "metadata": {},
   "source": [
    "### File Class Object Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbca32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class File:\n",
    "    def __init__(\n",
    "            self,\n",
    "            hbn : str,\n",
    "            main_title : str, \n",
    "            session_number : str, \n",
    "            significance : str, \n",
    "            date_filed : str, \n",
    "            principal_authors : str, \n",
    "            date_read : str, \n",
    "            primary_referral : str, \n",
    "            bill_status : str,  \n",
    "            text_filed : str, \n",
    "            is_file_downloadable : str\n",
    "            ):\n",
    "        self.hbn = hbn\n",
    "        self.main_title = main_title\n",
    "        self.session_number = session_number\n",
    "        self.significance = significance\n",
    "        self.date_filed = date_filed\n",
    "        self.principal_authors = principal_authors\n",
    "        self.date_read = date_read\n",
    "        self.primary_referral = primary_referral\n",
    "        self.bill_status = bill_status\n",
    "        self.text_filed = text_filed\n",
    "        self.is_file_downloadable = is_file_downloadable\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, File):\n",
    "            return self.hbn == other.hbn\n",
    "        return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.hbn)\n",
    "files : set[File] = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7abba",
   "metadata": {},
   "source": [
    "### JSON Encoder and Progress Loading for File Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b263247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_encoder(obj: File):\n",
    "    \"\"\"\n",
    "    Encodes FIle class instance to JSON instance \n",
    "\n",
    "    Args:\n",
    "        obj (File): File object\n",
    "\n",
    "    Raises:\n",
    "        TypeError: Occurs when object passed is not an instance of the File class\n",
    "\n",
    "    Returns:\n",
    "        dict[str,str]: dictionary for JSON parsing\n",
    "    \"\"\"\n",
    "    if isinstance(obj, File):\n",
    "        return {\n",
    "            'House Bill Number' : obj.hbn,\n",
    "            'Main Title' : obj.main_title,\n",
    "            'Session Number' : obj.session_number,\n",
    "            'Significance' : obj.significance,\n",
    "            'Date Filed' : obj.date_filed,\n",
    "            'Principal Authors' : obj.principal_authors,\n",
    "            'Date Read' : obj.date_read,\n",
    "            'Primary Referral' : obj.primary_referral,\n",
    "            'Bill Status' : obj.bill_status,\n",
    "            'Text Filed' : obj.text_filed\n",
    "        }\n",
    "    raise TypeError(\"Object is not JSON parsable.\")\n",
    "\n",
    "def load_files_from_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            for item in data:\n",
    "                # Reconstruct the File object using the JSON keys\n",
    "                # We use .get() to avoid errors if a key is missing\n",
    "                new_file = File(\n",
    "                    hbn=item.get(\"House Bill Number\"),\n",
    "                    main_title=item.get(\"Main Title\"),\n",
    "                    session_number=item.get(\"Session Number\"),\n",
    "                    significance=item.get(\"Significance\"),\n",
    "                    date_filed=item.get(\"Date Filed\"),\n",
    "                    principal_authors=item.get(\"Principal Authors\"),\n",
    "                    date_read=item.get(\"Date Read\"),\n",
    "                    primary_referral=item.get(\"Primary Referral\"),\n",
    "                    bill_status=item.get(\"Bill Status\"),\n",
    "                    text_filed=item.get(\"Text Filed\"),\n",
    "                    is_file_downloadable=item.get(\"Downloaded\", False)\n",
    "                )\n",
    "                files.add(new_file)\n",
    "        print(f\"Successfully loaded {len(files)} unique bills.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No existing JSON found. Starting with an empty set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fc34a",
   "metadata": {},
   "source": [
    "### Download File From URL Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08df14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, dest_folder: str):\n",
    "    \"\"\"\n",
    "    Downloads the file from the URL provided and places it in the destination folder provided\n",
    "\n",
    "    Inputs:\n",
    "    url (str): input URL of file\n",
    "    dest_folder (str): destination folder/directory of downloaded file\n",
    "    \n",
    "    Outputs:\n",
    "    Returns 1 if the download was successful, and 0 if not.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)  # create folder if it does not exist\n",
    "    try:\n",
    "        filename = url.split('/')[-1].replace(\" \", \"_\")  \n",
    "        file_path = os.path.join(dest_folder, filename)\n",
    "        # print(f\"URL: {url}\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.ok:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 8):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        f.flush()\n",
    "                        os.fsync(f.fileno())\n",
    "        else:  \n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370dff8",
   "metadata": {},
   "source": [
    "### Get Files from Current Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc60a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_files_from_page(hb_items_locator):\n",
    "    \"\"\"\n",
    "    Gets all house bill files from the current page\n",
    "\n",
    "    Args:\n",
    "        hb_items_locator (list[Locator]): list of scraped house bill divs\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    count = await hb_items_locator.count()\n",
    "    for i in range(count):\n",
    "        hb_item = hb_items_locator.nth(i)\n",
    "        \n",
    "        # Trigger AOS animation\n",
    "        await hb_item.scroll_into_view_if_needed()\n",
    "\n",
    "        # Get Header Info\n",
    "        hbn = await hb_item.locator(\"span.rounded.border span span\").first.inner_text()\n",
    "        main_title = await hb_item.locator(\"span.text-blue-500\").first.inner_text()\n",
    "\n",
    "        # Metadata Retrieval\n",
    "        async def get_meta(label):\n",
    "            try:\n",
    "                # Logic: Find the label div, then get the very next div sibling (+)\n",
    "                # only if it is inside the grid container\n",
    "                value_locator = hb_item.locator(\".grid.gap-1.px-5\") \\\n",
    "                                       .locator(f\"div:has-text('{label}') + div\")\n",
    "                \n",
    "                text = await value_locator.first.inner_text(timeout=1000)\n",
    "                return text.strip()\n",
    "            except:\n",
    "                return \"N/A\"\n",
    "\n",
    "        # Check if file already exists in metadata\n",
    "        if File(hbn=hbn, **dict.fromkeys(['main_title', 'session_number', 'significance', 'date_filed', 'principal_authors', 'date_read', 'primary_referral', 'bill_status', 'text_filed', 'is_file_downloadable'], \"N/A\")) in files:\n",
    "            print(f\"Skipping {hbn}: Already in database.\")\n",
    "            continue\n",
    "        \n",
    "        # PDF Link\n",
    "        pdf_loc = hb_item.locator('a[href$=\".pdf\"]').first\n",
    "        link = await pdf_loc.get_attribute('href') if await pdf_loc.count() > 0 else \"N/A\"\n",
    "        downloadability = False\n",
    "        if link != 'N/A':\n",
    "            downloadability = download(link, \"outputs/\")\n",
    "        # Build File Object\n",
    "\n",
    "        new_file = File(\n",
    "            hbn.strip(),\n",
    "            main_title.strip(),\n",
    "            await get_meta(\"Session No. :\"),\n",
    "            await get_meta(\"Significance :\"),\n",
    "            await get_meta(\"Date Filed :\"),\n",
    "            await get_meta(\"Principal Author/s :\"),\n",
    "            await get_meta(\"Date Read :\"),\n",
    "            await get_meta(\"Primary Referral :\"),\n",
    "            await get_meta(\"Bill Status :\"),\n",
    "            link,\n",
    "            downloadability # Downloadable\n",
    "        )\n",
    "        files.add(new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75642a",
   "metadata": {},
   "source": [
    "## Scraper Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ec883",
   "metadata": {},
   "source": [
    "### Actual Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b5b3c",
   "metadata": {},
   "source": [
    "#### File Metadata Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76f815bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Collision Check\n",
      "Successfully loaded 29 unique bills.\n"
     ]
    }
   ],
   "source": [
    "load_files_from_json('outputs/metadata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f514dbc",
   "metadata": {},
   "source": [
    "#### Scraper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89fca00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading GeoIP database: 100%|██████████| 63.1M/63.1M [00:01<00:00, 53.7MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred. Saving progress...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    async with AsyncCamoufox(headless=False, geoip=True) as browser:\n",
    "        context = await browser.new_context(viewport={\"width\":1000, \"height\":500})\n",
    "        page = await context.new_page()\n",
    "\n",
    "        await page.goto(\"https://congress.gov.ph/legislative-documents/\")\n",
    "        \n",
    "        # Wait for initial load\n",
    "        await page.wait_for_selector('[id=\"20th Congress\"]', state='visible', timeout=90000)\n",
    "        \n",
    "        # Set pagination to 100\n",
    "        await page.locator(\"select.form-select\").nth(1).select_option('100')    \n",
    "        \n",
    "        # Open dropdown\n",
    "        await page.locator('[id=\"20th Congress\"]').click()\n",
    "        \n",
    "        # Initial scroll and wait for first page items\n",
    "        await page.wait_for_selector('.cursor-pointer.rounded-sm.border', state='visible')\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\n",
    "        # Skip pages if needed based on files array\n",
    "        idx = 1\n",
    "        for i in range(0, int(len(files) / 100)):\n",
    "            print(f\"Skipping page {i}\\n\")\n",
    "            old_bill_id = await page.locator(\".cursor-pointer span.rounded.border span span\").first.inner_text()\n",
    "            next_button = page.locator('li.next:not(.disabled) a') # Specifically target the 'Next' link\n",
    "            await next_button.click()\n",
    "            try:\n",
    "                await page.wait_for_function(\n",
    "                    f\"\"\"() => {{\n",
    "                        const el = document.querySelector(\".cursor-pointer span.rounded.border span span\");\n",
    "                        return el && el.innerText.trim() !== \"{old_bill_id.strip()}\";\n",
    "                    }}\"\"\",\n",
    "                    timeout=15000 # 15 seconds is usually enough for a data swap\n",
    "                )\n",
    "            except:\n",
    "                # Fallback if JS check fails: wait for network to settle\n",
    "                await page.wait_for_load_state(\"networkidle\")\n",
    "            \n",
    "            # Small buffer for the UI to stabilize\n",
    "            await page.wait_for_timeout(3000)\n",
    "            idx += 1\n",
    "\n",
    "        while(idx <= 79):\n",
    "            # Scrape current page\n",
    "            hb_items_locator = page.locator('.cursor-pointer.rounded-sm.border')\n",
    "            await get_files_from_page(hb_items_locator)\n",
    "            \n",
    "            print(f\"Finished scraping page {idx}\")\n",
    "\n",
    "            # Get \"Next\" button locator\n",
    "            next_button = page.locator('li.next:not(.disabled) a') \n",
    "            \n",
    "            if await next_button.count() > 0:\n",
    "                # Capture ID of the first item to track when the data actually changes\n",
    "                old_bill_id = await page.locator(\".cursor-pointer span.rounded.border span span\").first.inner_text()\n",
    "                \n",
    "                # Click next button\n",
    "                await next_button.click()\n",
    "\n",
    "                # 4. Wait for Content Refresh from Page Change\n",
    "                try:\n",
    "                    await page.wait_for_function(\n",
    "                        f\"\"\"() => {{\n",
    "                            const el = document.querySelector(\".cursor-pointer span.rounded.border span span\");\n",
    "                            return el && el.innerText.trim() !== \"{old_bill_id.strip()}\";\n",
    "                        }}\"\"\",\n",
    "                        timeout=15000 # timeout buffer\n",
    "                    )\n",
    "                except:\n",
    "                    # Fallback if JS check fails: wait for network to settle\n",
    "                    await page.wait_for_load_state(\"networkidle\")\n",
    "                \n",
    "                # Small buffer for the UI to stabilize\n",
    "                await page.wait_for_timeout(3000)\n",
    "                idx += 1\n",
    "            else:\n",
    "                print(\"No more pages available.\")\n",
    "                break\n",
    "except:\n",
    "    print(\"Error occurred. Saving progress...\")\n",
    "\n",
    "# Processing logic (e.g., saving to JSON)\n",
    "with open('outputs/metadata.json', mode='w', encoding='utf-8') as f:\n",
    "    json.dump(\n",
    "        obj=list(files),\n",
    "        fp=f,\n",
    "        default=json_encoder,\n",
    "        indent=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5ef58",
   "metadata": {},
   "source": [
    "### Upload to AWS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0efcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unknown options: --recursive\n",
      "(dryrun) upload: outputs/metadata.json to s3://aaia-raw/congress/20th_congress/metadata/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Data Sync to S3 Bucket\n",
    "!aws s3 sync outputs/ {AWS_BUCKET_DATA_LOCATION} --dryrun --exclude \"*\" --include \"*.pdf\"\n",
    "\n",
    "# Metadata upload to S3\n",
    "!aws s3 cp outputs/metadata.json {AWS_BUCKET_METADATA_LOCATION} --dryrun \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congress-house-bills-scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
