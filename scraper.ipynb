{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e947ce97",
   "metadata": {},
   "source": [
    "# Congress House Bills Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39836aad",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab104ae",
   "metadata": {},
   "source": [
    "A web scraper for Congress House bills (HBNXXXXX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7153d4",
   "metadata": {},
   "source": [
    "## Imports and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b60d2",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6990c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m72 packages\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c375f3",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b39ab126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchright.async_api import async_playwright, Page, Browser\n",
    "from camoufox.async_api import AsyncCamoufox\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1ee89",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "418729f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_BUCKET_DATA_LOCATION = os.getenv(\"AWS_BUCKET_DATA_LOCATION\")\n",
    "AWS_BUCKET_METADATA_LOCATION = os.getenv(\"AWS_BUCKET_METADATA_LOCATION\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7f814",
   "metadata": {},
   "source": [
    "## Helper and Functions and Class Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f4617",
   "metadata": {},
   "source": [
    "### File Class Object Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbbca32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class File:\n",
    "    def __init__(\n",
    "            self,\n",
    "            hbn : str,\n",
    "            main_title : str, \n",
    "            session_number : str, \n",
    "            significance : str, \n",
    "            date_filed : str, \n",
    "            principal_authors : str, \n",
    "            date_read : str, \n",
    "            primary_referral : str, \n",
    "            bill_status : str,  \n",
    "            text_filed : str, \n",
    "            is_file_downloadable : str\n",
    "            ):\n",
    "        self.hbn = hbn\n",
    "        self.main_title = main_title\n",
    "        self.session_number = session_number\n",
    "        self.significance = significance\n",
    "        self.date_filed = date_filed\n",
    "        self.principal_authors = principal_authors\n",
    "        self.date_read = date_read\n",
    "        self.primary_referral = primary_referral\n",
    "        self.bill_status = bill_status\n",
    "        self.text_filed = text_filed\n",
    "        self.is_file_downloadable = is_file_downloadable\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, File):\n",
    "            return self.hbn == other.hbn\n",
    "        return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.hbn)\n",
    "files : set[File] = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7abba",
   "metadata": {},
   "source": [
    "### JSON Encoder and Progress Loading for File Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b263247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_encoder(obj: File):\n",
    "    \"\"\"\n",
    "    Encodes FIle class instance to JSON instance \n",
    "\n",
    "    Args:\n",
    "        obj (File): File object\n",
    "\n",
    "    Raises:\n",
    "        TypeError: Occurs when object passed is not an instance of the File class\n",
    "\n",
    "    Returns:\n",
    "        dict[str,str]: dictionary for JSON parsing\n",
    "    \"\"\"\n",
    "    if isinstance(obj, File):\n",
    "        return {\n",
    "            'House Bill Number' : obj.hbn,\n",
    "            'Main Title' : obj.main_title,\n",
    "            'Session Number' : obj.session_number,\n",
    "            'Significance' : obj.significance,\n",
    "            'Date Filed' : obj.date_filed,\n",
    "            'Principal Authors' : obj.principal_authors,\n",
    "            'Date Read' : obj.date_read,\n",
    "            'Primary Referral' : obj.primary_referral,\n",
    "            'Bill Status' : obj.bill_status,\n",
    "            'Text Filed' : obj.text_filed\n",
    "        }\n",
    "    raise TypeError(\"Object is not JSON parsable.\")\n",
    "\n",
    "def load_files_from_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            for item in data:\n",
    "                # Reconstruct the File object using the JSON keys\n",
    "                # We use .get() to avoid errors if a key is missing\n",
    "                new_file = File(\n",
    "                    hbn=item.get(\"House Bill Number\"),\n",
    "                    main_title=item.get(\"Main Title\"),\n",
    "                    session_number=item.get(\"Session Number\"),\n",
    "                    significance=item.get(\"Significance\"),\n",
    "                    date_filed=item.get(\"Date Filed\"),\n",
    "                    principal_authors=item.get(\"Principal Authors\"),\n",
    "                    date_read=item.get(\"Date Read\"),\n",
    "                    primary_referral=item.get(\"Primary Referral\"),\n",
    "                    bill_status=item.get(\"Bill Status\"),\n",
    "                    text_filed=item.get(\"Text Filed\"),\n",
    "                    is_file_downloadable=item.get(\"Downloaded\", False)\n",
    "                )\n",
    "                files.add(new_file)\n",
    "        print(f\"Successfully loaded {len(files)} unique bills.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No existing JSON found. Starting with an empty set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fc34a",
   "metadata": {},
   "source": [
    "### Download File From URL Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08df14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, dest_folder: str):\n",
    "    \"\"\"\n",
    "    Downloads the file from the URL provided and places it in the destination folder provided\n",
    "\n",
    "    Inputs:\n",
    "    url (str): input URL of file\n",
    "    dest_folder (str): destination folder/directory of downloaded file\n",
    "    \n",
    "    Outputs:\n",
    "    Returns 1 if the download was successful, and 0 if not.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)  # create folder if it does not exist\n",
    "    try:\n",
    "        filename = url.split('/')[-1].replace(\" \", \"_\")  \n",
    "        file_path = os.path.join(dest_folder, filename)\n",
    "        # print(f\"URL: {url}\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.ok:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 8):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        f.flush()\n",
    "                        os.fsync(f.fileno())\n",
    "        else:  \n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370dff8",
   "metadata": {},
   "source": [
    "### Get Files from Current Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8dc60a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_files_from_page(hb_items_locator):\n",
    "    \"\"\"\n",
    "    Gets all house bill files from the current page\n",
    "\n",
    "    Args:\n",
    "        hb_items_locator (list[Locator]): list of scraped house bill divs\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    count = await hb_items_locator.count()\n",
    "    for i in range(count):\n",
    "        hb_item = hb_items_locator.nth(i)\n",
    "        \n",
    "        # Trigger AOS animation\n",
    "        await hb_item.scroll_into_view_if_needed()\n",
    "\n",
    "        # Get Header Info\n",
    "        hbn = await hb_item.locator(\"span.rounded.border span span\").first.inner_text()\n",
    "        main_title = await hb_item.locator(\"span.text-blue-500\").first.inner_text()\n",
    "\n",
    "        # Metadata Retrieval\n",
    "        async def get_meta(label):\n",
    "            try:\n",
    "                # Logic: Find the label div, then get the very next div sibling (+)\n",
    "                # only if it is inside the grid container\n",
    "                value_locator = hb_item.locator(\".grid.gap-1.px-5\") \\\n",
    "                                       .locator(f\"div:has-text('{label}') + div\")\n",
    "                \n",
    "                text = await value_locator.first.inner_text(timeout=1000)\n",
    "                return text.strip()\n",
    "            except:\n",
    "                return \"N/A\"\n",
    "\n",
    "        # Check if file already exists in metadata\n",
    "        is_hb_in_metadata = False\n",
    "        matched_hb = next((u for u in files if u.hbn == hbn), None)\n",
    "        if matched_hb:\n",
    "            # print(f\"{hbn}: Already in database. Checking for changes...\")\n",
    "            is_hb_in_metadata = True        \n",
    "\n",
    "        # PDF Link\n",
    "        pdf_loc = hb_item.locator('a[href$=\".pdf\"]').first\n",
    "        link = await pdf_loc.get_attribute('href') if await pdf_loc.count() > 0 else \"N/A\"\n",
    "        downloadability = False\n",
    "        if link != 'N/A' and not is_hb_in_metadata:\n",
    "            print(f\"Downloading file: {hbn}...\")\n",
    "            downloadability = download(link, \"outputs/\")\n",
    "\n",
    "        # Build File Object\n",
    "        new_file = File(\n",
    "            hbn.strip(),\n",
    "            main_title.strip(),\n",
    "            await get_meta(\"Session No. :\"),\n",
    "            await get_meta(\"Significance :\"),\n",
    "            await get_meta(\"Date Filed :\"),\n",
    "            await get_meta(\"Principal Author/s :\"),\n",
    "            await get_meta(\"Date Read :\"),\n",
    "            await get_meta(\"Primary Referral :\"),\n",
    "            await get_meta(\"Bill Status :\"),\n",
    "            link,\n",
    "            downloadability # Downloadable\n",
    "        )\n",
    "\n",
    "        # Check for metadata changes if the file already exists\n",
    "        if is_hb_in_metadata:\n",
    "            # Check for changes with safeguard for N/A change due to possible bugs\n",
    "            if new_file.bill_status != matched_hb.bill_status and new_file.bill_status != 'N/A':\n",
    "                print(f\"Change in Bill Status found. Updating bill status for {hbn}...\")\n",
    "                print(f\"[LOG]\\n   Old Bill Status: {matched_hb.bill_status}\\n   New Bill Status: {new_file.bill_status}\")\n",
    "                matched_hb.bill_status = new_file.bill_status\n",
    "        else:\n",
    "            files.add(new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75642a",
   "metadata": {},
   "source": [
    "## Scraper Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ec883",
   "metadata": {},
   "source": [
    "### Actual Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b5b3c",
   "metadata": {},
   "source": [
    "#### File Metadata Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76f815bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 7894 unique bills.\n"
     ]
    }
   ],
   "source": [
    "load_files_from_json('outputs/metadata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f514dbc",
   "metadata": {},
   "source": [
    "#### Scraper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89fca00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1\n",
      "Finished scraping page 2\n",
      "Finished scraping page 3\n",
      "Change in Bill Status found. Updating bill status for HB00301...\n",
      "[LOG]\n",
      "   Old Bill Status: Pending with the Committee on PUBLIC WORKS AND HIGHWAYS since 2025-07-29\n",
      "   New Bill Status: Pending with the Committee on HEALTH since 2025-07-29\n",
      "Finished scraping page 4\n",
      "Finished scraping page 5\n",
      "Finished scraping page 6\n",
      "Finished scraping page 7\n",
      "Finished scraping page 8\n",
      "Finished scraping page 9\n",
      "Finished scraping page 10\n",
      "Finished scraping page 11\n",
      "Finished scraping page 12\n",
      "Finished scraping page 13\n",
      "Finished scraping page 14\n",
      "Finished scraping page 15\n",
      "Finished scraping page 16\n",
      "Finished scraping page 17\n",
      "Finished scraping page 18\n",
      "Finished scraping page 19\n",
      "Finished scraping page 20\n",
      "Finished scraping page 21\n",
      "Finished scraping page 22\n",
      "Finished scraping page 23\n",
      "Finished scraping page 24\n",
      "Finished scraping page 25\n",
      "Finished scraping page 26\n",
      "Finished scraping page 27\n",
      "Finished scraping page 28\n",
      "Finished scraping page 29\n",
      "Finished scraping page 30\n",
      "Finished scraping page 31\n",
      "Finished scraping page 32\n",
      "Finished scraping page 33\n",
      "Finished scraping page 34\n",
      "Finished scraping page 35\n",
      "Finished scraping page 36\n",
      "Finished scraping page 37\n",
      "Finished scraping page 38\n",
      "Finished scraping page 39\n",
      "Finished scraping page 40\n",
      "Finished scraping page 41\n",
      "Finished scraping page 42\n",
      "Finished scraping page 43\n",
      "Finished scraping page 44\n",
      "Finished scraping page 45\n",
      "Finished scraping page 46\n",
      "Finished scraping page 47\n",
      "Finished scraping page 48\n",
      "Finished scraping page 49\n",
      "Finished scraping page 50\n",
      "Finished scraping page 51\n",
      "Finished scraping page 52\n",
      "Finished scraping page 53\n",
      "Finished scraping page 54\n",
      "Finished scraping page 55\n",
      "Finished scraping page 56\n",
      "Finished scraping page 57\n",
      "Finished scraping page 58\n",
      "Finished scraping page 59\n",
      "Finished scraping page 60\n",
      "Finished scraping page 61\n",
      "Finished scraping page 62\n",
      "Finished scraping page 63\n",
      "Finished scraping page 64\n",
      "Finished scraping page 65\n",
      "Finished scraping page 66\n",
      "Finished scraping page 67\n",
      "Finished scraping page 68\n",
      "Finished scraping page 69\n",
      "Finished scraping page 70\n",
      "Finished scraping page 71\n",
      "Finished scraping page 72\n",
      "Finished scraping page 73\n",
      "Finished scraping page 74\n",
      "Finished scraping page 75\n",
      "Finished scraping page 76\n",
      "Finished scraping page 77\n",
      "Finished scraping page 78\n",
      "Finished scraping page 79\n",
      "No more pages available.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    async with AsyncCamoufox(headless=False, geoip=True) as browser:\n",
    "        context = await browser.new_context(viewport={\"width\":1000, \"height\":500})\n",
    "        page = await context.new_page()\n",
    "\n",
    "        await page.goto(\"https://congress.gov.ph/legislative-documents/\")\n",
    "        \n",
    "        # Wait for initial load\n",
    "        await page.wait_for_selector('[id=\"20th Congress\"]', state='visible', timeout=90000)\n",
    "        \n",
    "        # Set pagination to 100\n",
    "        await page.locator(\"select.form-select\").nth(1).select_option('100')    \n",
    "        \n",
    "        # Open dropdown\n",
    "        await page.locator('[id=\"20th Congress\"]').click()\n",
    "        \n",
    "        # Initial scroll and wait for first page items\n",
    "        await page.wait_for_selector('.cursor-pointer.rounded-sm.border', state='visible')\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\n",
    "        # Skip pages if needed based on files array\n",
    "        idx = 1\n",
    "        \n",
    "        while(idx <= 79):\n",
    "            # Scrape current page\n",
    "            hb_items_locator = page.locator('.cursor-pointer.rounded-sm.border')\n",
    "            await get_files_from_page(hb_items_locator)\n",
    "            \n",
    "            print(f\"Finished scraping page {idx}\")\n",
    "\n",
    "            # Get \"Next\" button locator\n",
    "            next_button = page.locator('li.next:not(.disabled) a') \n",
    "            \n",
    "            if await next_button.count() > 0:\n",
    "                # Capture ID of the first item to track when the data actually changes\n",
    "                old_bill_id = await page.locator(\".cursor-pointer span.rounded.border span span\").first.inner_text()\n",
    "                \n",
    "                # Click next button\n",
    "                await next_button.click()\n",
    "\n",
    "                # 4. Wait for Content Refresh from Page Change\n",
    "                try:\n",
    "                    await page.wait_for_function(\n",
    "                        f\"\"\"() => {{\n",
    "                            const el = document.querySelector(\".cursor-pointer span.rounded.border span span\");\n",
    "                            return el && el.innerText.trim() !== \"{old_bill_id.strip()}\";\n",
    "                        }}\"\"\",\n",
    "                        timeout=15000 # timeout buffer\n",
    "                    )\n",
    "                except:\n",
    "                    # Fallback if JS check fails: wait for network to settle\n",
    "                    await page.wait_for_load_state(\"networkidle\")\n",
    "                \n",
    "                # Small buffer for the UI to stabilize\n",
    "                await page.wait_for_timeout(3000)\n",
    "                idx += 1\n",
    "            else:\n",
    "                print(\"No more pages available.\")\n",
    "                break\n",
    "except:\n",
    "    print(\"Error occurred. Saving progress...\")\n",
    "\n",
    "# Processing logic (e.g., saving to JSON)\n",
    "with open('outputs/metadata.json', mode='w', encoding='utf-8') as f:\n",
    "    json.dump(\n",
    "        obj=list(files),\n",
    "        fp=f,\n",
    "        default=json_encoder,\n",
    "        indent=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5ef58",
   "metadata": {},
   "source": [
    "### Upload to AWS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0efcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: outputs/metadata.json to s3://aaia-raw/congress/20th_congress/metadata/metadata.json\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Data Sync to S3 Bucket\n",
    "# !aws s3 sync outputs/ {AWS_BUCKET_DATA_LOCATION} --dryrun --exclude \"*\" --include \"*.pdf\"\n",
    "\n",
    "# Metadata upload to S3\n",
    "!aws s3 cp outputs/metadata.json {AWS_BUCKET_METADATA_LOCATION}  \n",
    "print(AWS_BUCKET_METADATA_LOCATION)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congress-house-bills-scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
